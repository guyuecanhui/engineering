# 异常检测

异常检测 (anomaly detection)，或者又被称为“离群点检测” (outlier detection)，是机器学习研究领域中跟现实紧密联系、有广泛应用需求的一类问题。但是，什么是异常，并没有标准答案，通常因具体应用场景而异。如果要给一个比较通用的定义，很多文献通常会引用 Hawkins 在文章开头那段话。很多后来者的说法，跟这个定义大同小异。这些定义虽然笼统，但其实暗含了认定“异常”的两个标准或者说假设：

* 异常数据跟样本中大多数数据不太一样。
* 异常数据在整体数据样本中占比比较小。

为了刻画异常数据的“不一样”，最直接的做法是利用各种统计的、距离的、密度的量化指标去描述数据样本跟其他样本的疏离程度。

---
<!--sec data-title="异常检测分类" data-id="anomaly_00" data-show=true ces-->

### 一、按检测方法分

#### 1.1. 基于统计的异常检测方法
该方法的基本步骤是对数据点进行建模，再以假定的模型（如泊松分布、正太分布等）根据点的分布来确定是否异常。这种方法首先需要对数据的分布有所了解，进而通过数据变异指标来发现异常数据。常用变异指标有极差、四分位数间距、均差、标准差、变异系数等。但是，基于统计的方法检测出来的异常点产生机制可能不唯一，而且它在很大程度上依赖于待挖掘的数据集是否满足某种概率分布模型，另外模型的参数、离群点的数目等都非常重要，确定这些因素通常都比较困难。因此，实际情况中算法的应用性和可移植性较差。

#### 1.2. 基于距离的异常检测方法
该方法定义包含并拓展了基于统计的思想，即使数据集不满足任何特定分布模型，它仍能有效地发现离群点，特别是当空间维数比较高时，算法的效率比基于密度的方法要高得多 。算法具体实现时，首先给出记录数据点间的距离（如 曼哈顿距离 、欧氏距离等），然后对数据进行一定的预处理以后就可以根据距离的定义来检测异常值。如基于K-Means的聚类可以将离每个类中心点最远或者不属于任何一个类的数据点提取出来而发现异常值。基于距离的离群检测方法不需要用户拥有任何领域知识且具有比较直观的意义，算法比较容易理解，因此在实际中应用得比较多。

#### 1.3. 基于密度的离群检测方法
这种方法一般都建立在距离的基础上，其主要思想是将数据点之间的距离和某一范围内数据数这两个参数结合起来，从而得到“密度”的概念，然后根据密度判定记录是否为离群点。例如LOF（局部异常因子）就是用于识别基于密度的局部异常值的算法。离群点被定义为相对于全局的局部离群点，这与传统异常点的定义不同，异常点不再是一个二值属性（要么是异常点，要么是正常点，实际上的定义类似于98%的可能性是一个异常点），它摈弃了以前所有的异常定义中非此即彼的绝对异常观念，更加符合现实生活中的应用；但其缺点就是它只对数值数据有效。

#### 1.4. 基于偏移的异常点检测方法
基于偏移的离群检测算法 (Deviation-based Outlier Detection) 通过对测试数据集主要特征的检验来发现离群点。目前，基于偏移的检测算法大多都停留在理论研究上，实际应用比较少。

#### 1.5. 基于时间序列的异常点监测方法
所谓时间序列就是将某一指标在不同时间上的数值，按照时间先后顺序排序而成的数列。这种数列虽然由于受到各种偶然因素的影响而表现出某种随机性，不可能完全准确地用历史值来预测将来，但是前后时刻的数值或数据点的相关性往往呈现某种趋势性或周期性变化，这是时间序列挖掘的可行性之所在。时间序列中没有具体描述被研究现象与其影响因素之间的关系，而是把各影响因素分别看作一种作用力，被研究对象的时间序列则看成合力；然后按作用特点和影响效果将影响因素规为 4 类，即趋势变动（ T ）、季节变动（ S ）、循环变动（ C ）和随机变动（ I ）。这四种类项的变动叠加在一起，形成了实际观测到的时间序列，因而可以通过对这四种变动形式的考察来研究时间系列的变动。目前国际和国内对时间序列相似度的研究提出了许多种解决方法，这些方法主要包括基于直接距离、傅立叶变换、 ARMA 模型参数法、规范变换、时间弯曲模型、界标模型、神经网络、小波变换、规则推导等。 关于时间序列的异常检测应用案例，可参照[统计学在点击流数据中的应用范例——Adobe Analytics异常检测](http://www.searchmarketingart.com/adobe-analytics-anomaly-detection.html)。

### 二、根据原始数据集分类：
#### 2.1 新奇检测（Novelty Detection）
新奇检测的前提是已知训练数据集是“纯净”的，未被真正的“噪音”数据或真实的“离群点”污染，然后针对这些数据训练完成之后再对新的数据进行训练以寻找异常数据。检测算法例如[one-class SVM](http://www.dataivy.cn/blog/%E6%96%B0%E5%A5%87%E6%A3%80%E6%B5%8Bnovelty-detection/)。

#### 2.2 离群点检测（Outlier Detection）
离群点检测的训练数据集则包含“离群点”数据（这是与新奇点检测最大的差别），对这些数据训练完成之后再在新的数据集中寻找异常数据，检测算法例如[EllipticEnvelope](http://www.dataivy.cn/blog/%E7%A6%BB%E7%BE%A4%E7%82%B9%E6%A3%80%E6%B5%8Boutlier-detection/)
<!--endsec-->

<!--sec data-title="Isolation Forest检测算法" data-id="anomaly_0" data-show=true ces-->
### 主要思想
对一组数据进行随机切分（在某个特征的取值范围内随机选择一个值作为切分点），离群的点只需要较少次数的切分就能切分出来。根据这种思想，可以采用二叉树去对数据进行切分，数据点在二叉树中所处的深度反应了该条数据的“疏离”程度。

### 算法描述
算法分为训练和预测两部分：

**训练**：即抽取多个样本，构建多棵二叉树（Isolation Tree，即 iTree）。
```
1. 从全量数据中抽取一批样本，用于构建一棵iTree;
2. 随机选择一个特征作为起始节点，并在该特征的最大值和最小值之间随机选择一个值，将样本中小于该取值的数据划到左分支，大于等于该取值的划到右分支;
3. 在左右两个分支数据中，重复步骤2，直到 a) 数据不可再分，即只包含一条数据，或者全部数据相同; 或 b) 二叉树达到限定的最大深度;
4. 重复步骤1-3直到达到预设的iTree个数;
```

**预测**：综合多棵iTree的结果，计算每个数据点x的异常分值。对于某个iTree，假设x所在的叶节点有n条数据：
$$
H(x)=e+C(n)
$$
其中，$$e$$ 为x从iTree的根节点到叶节点过程中经过的边的数目，$$C(n)$$ 可以认为是一个修正值，它表示在一棵用n条样本数据构建的二叉树的平均路径长度。
$$
C(n)=2H(n-1)-\frac{2(n-1)}{n}
$$
其中，$$H(n-1) \approx \ln(n-1)+0.5772156649$$。
$$
S(x)=2^{-\frac{E(h(x))}{C(\psi)}}
$$
其中，$$E(h(x))$$ 表示数据 x 在多棵 iTree 的路径长度的均值，$$\psi$$ 表示单棵 iTree 的训练样本的样本数，$$C(\psi)$$ 表示用 $$\psi$$ 条数据构建的二叉树的平均路径长度，它在这里主要用来做归一化。 
```
计算x在每棵iTree上的路径长度H(x);
计算x的最终异常分值S(x);
```
从异常分值的公式看，如果数据 x 在多棵 iTree 中的平均路径长度越短，得分越接近 1，表明数据 x 越异常；如果数据 x 在多棵 iTree 中的平均路径长度越长，得分越接近 0，表示数据 x 越正常；如果数据 x 在多棵 iTree 中的平均路径长度接近整体均值，则打分会在 0.5 附近。 

### 算法应用
Isolation Forest 算法主要有两个参数：一个是二叉树的个数；另一个是训练单棵 iTree 时候抽取样本的数目。实验表明，当设定为 100 棵树，抽样样本数为 256 条时候，IF 在大多数情况下就已经可以取得不错的效果。这也体现了算法的简单、高效。

Isolation Forest 是无监督的异常检测算法，在实际应用时，并不需要黑白标签。需要注意的是：

1. 如果训练样本中异常样本的比例比较高，违背了先前提到的异常检测的基本假设，可能最终的效果会受影响；
2. 异常检测跟具体的应用场景紧密相关，算法检测出的“异常”不一定是我们实际想要的。比如，在识别虚假交易时，异常的交易未必就是虚假的交易。所以，在特征选择时，可能需要过滤不太相关的特征，以免识别出一些不太相关的“异常”。 

<!--endsec-->
<!--sec data-title="Mann-Whitney U 单侧检验算法" data-id="anomaly_1" data-show=true ces-->

### 主要思想
Mann-Whitney U 检验是用得最广泛的两独立样本秩和检验方法。简单的说，该检验是与独立样本t检验相对应的方法，当正态分布、方差齐性等不能达到t检验的要求时，可以使用该检验。其假设基础是：若两个样本有差异，则他们的中心位置将不同。

在异常检测中，异常数据存在一定的模式。例如对于时序的日志数据，当系统发生异常时，日志的频率将大大增加，因此主要利用Mann-Whitney U单侧检验，即检验时序数据的间隔是否减小（即日志打印频率是否增加），进而判定系统是否发生异常。

### 算法描述
以时序数据 $$D={d_1,d_2,\cdots,d_{20}}$$ 为例，假设这些时序数据带有时间戳，计算时主要考虑它们的时间戳信息：
1. 将时序数据按时间顺序排列：$$d_1,d_2,d_3,\cdots,d_{20}$$；
2. 计算时序数据两两之间的间隔：$$t_{1,2},t_{2,3},\cdots,t_{19,20}$$；
3. 将这些间隔分成两组：$$G_1:[t_{1,2}\sim t_{9,10}]$$ (元素数量 $$n_1=9$$)，$$G_2:[t_{10,11}\sim t_{19,20}]$$   (元素数量 $$n_2=10$$)；
4. 将这些间隔进行排序，从而求得它们的秩：$$G_1:[r_1\sim r_9],\ G_2:[r_{10}\sim r_{20}]$$；
5. 计算第一组数据的秩和及其U值：$$R_1=\sum_{r_i\in G_1}r_i,\ U_1=R_1-\frac{n_1(n_1+1)}{2}$$；
6. 由于数据量比较大，可以认为 $$U_1$$ 近似满足正态分布，其均值和方差为：$$E=\frac{n_1n_2}{2},D=\frac{n_1n_2(n_1+n_2+1)}{12}$$；
7. 将 $$R_1$$ 转换成标准正态分布，计算标准化值：$$Z=\frac{U_1-E}{D}$$；
8. 从负无穷到Z的累积概率可以看成是 $$G_1$$ 组数据大于 $$G_2$$ 组数据的置信程度，也就是时序数据频率加快的置信度，当该值大于0.9接受时序数据频率加快的假设；

### 算法解释
* 之所以称为U Test，主要就是因为这里最主要的变量为 $$U$$，当数据量小的时候可以查表比较，当数据量大的时候，近似满足正态分布，是判断假设是否成立的核心；
* 这里是单侧检验，因此只需要计算 $$U_1$$，双侧检验 (判断是否明显不同) 则要用 $$U_1,U_2$$ 的最小值来计算 $$Z$$ 值；
* $$U_1$$ 和 $$U_2$$ 的取值范围是 $$[0\sim n_1n_2]$$，且 $$U_1+U_2=n_1n_2$$，因此Z的取值范围为 $$[-\frac{n_1n_2}{2D}\sim\frac{n_1n_2}{2D}]$$；
* 不要求两组数据数量相同，因为U值会减去一个跟数量有关的正则量 $$\frac{n(n+1)}{2}$$；

### 算法应用
Mann-Whitney U 单侧检验算法可以应用于日志频率监控（判定日志频率是否增加），也可以用于资源使用率监控（判定资源使用率是否增加）等。

> Mann-Whitney U检验可以参考Wiki：https://en.wikipedia.org/wiki/Mann%E2%80%93Whitney_U_test；

<!--endsec-->

